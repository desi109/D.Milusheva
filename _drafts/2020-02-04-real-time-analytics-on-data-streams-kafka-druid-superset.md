---
title: 'Real Time Analytics On Data Streams: Kafka + Druid + Superset (part 1)'
last_modified_at: 2020-02-04T20:38:15+00:00
categories:
  - Blog
tags:
  - Post Formats
  - readability
  - standard
---

One popular trend in the data world recently is the rise of stream analytics. Organizations are increasingly driven to build solutions that can provide immediate access to key business intelligence insights through real-time data exploration. Architecting a data stack to transmit, store, and analyze streams at scale can be difficult, engineering feat without the right tools. Luckily, existing open source solutions can be combined to form a flexible and scalable streaming analytics stack. In this blog post, we will use Apache&nbsp;`{Kafka, Superset, Druid}`&nbsp;to set up a system that enables immediate exploration and visualization of event data.  
  
So now I want to show you an example of how to create interactive real time dashboards on data streams. But first, let me explain to you a little more about Kafka, Druid, and Superset.

## 

<div class="wp-block-image">
  <figure class="aligncenter size-large is-resized"><a href="https://www.dev-guide.org/wp-content/uploads/2020/02/948138d5a72dec2ed43744b495beb0895d00dc9a-1.png" target="_blank" rel="noreferrer noopener"><img loading="lazy" src="https://dev-guide.org/wp-content/uploads/2020/02/948138d5a72dec2ed43744b495beb0895d00dc9a-1.png" alt="" class="wp-image-86" width="380" height="112" srcset="https://www.dev-guide.org/wp-content/uploads/2020/02/948138d5a72dec2ed43744b495beb0895d00dc9a-1.png 749w, https://www.dev-guide.org/wp-content/uploads/2020/02/948138d5a72dec2ed43744b495beb0895d00dc9a-1-300x89.png 300w" sizes="(max-width: 380px) 100vw, 380px" /></a></figure>
</div>

<a rel="noreferrer noopener" href="https://kafka.apache.org/" target="_blank">Apache™ Kafka</a>&nbsp;is a publish-subscribe message bus that is designed for the delivery of streams. The architecture of Kafka is modeled as a distributed commit log, and Kafka provides resource isolation between things that produce data and things that consume data. Kafka is often used as a central repository of streams, where events are stored in Kafka for an intermediate period of time before they are routed elsewhere in a data cluster for further processing and analysis.  
  
In this setup, Kafka is used to collect and buffer the events, that are then ingested by Druid. We need Kafka to persist the data and act as a buffer when there are bursts of events, that happen.

## 

<div class="wp-block-image">
  <figure class="aligncenter size-large is-resized"><a href="https://www.dev-guide.org/wp-content/uploads/2020/02/0_lfuHdT9SFON6R2R5-1.png" target="_blank" rel="noreferrer noopener"><img loading="lazy" src="https://dev-guide.org/wp-content/uploads/2020/02/0_lfuHdT9SFON6R2R5-1.png" alt="" class="wp-image-89" width="274" height="74" srcset="https://www.dev-guide.org/wp-content/uploads/2020/02/0_lfuHdT9SFON6R2R5-1.png 600w, https://www.dev-guide.org/wp-content/uploads/2020/02/0_lfuHdT9SFON6R2R5-1-300x81.png 300w" sizes="(max-width: 274px) 100vw, 274px" /></a></figure>
</div>

<a rel="noreferrer noopener" href="https://github.com/apache/incubator-druid/" target="_blank">Apache™ Druid</a> is a real-time analytics database designed for fast slice-and-dice analytics on large data sets. It provides real-time data ingestion from Kafka, flexible data exploration, and fast data aggregation. For example, when the data is being generated by the users, or sensor, or whatever, it flows in the application landscape. Where it is processed and it flows into your database where it is directly available for querying. 

## 

<div class="wp-block-image is-style-default">
  <figure class="aligncenter size-large is-resized"><a href="https://www.dev-guide.org/wp-content/uploads/2020/02/Apache-superset-logo.png" target="_blank" rel="noreferrer noopener"><img loading="lazy" src="https://dev-guide.org/wp-content/uploads/2020/02/Apache-superset-logo.png" alt="" class="wp-image-90" width="334" height="167" /></a></figure>
</div>

<a rel="noreferrer noopener" href="https://github.com/apache/incubator-superset" target="_blank">Apache™ Superset</a> is a web application that provides an intuitive interface to explore and visualize datasets and create interactive dashboards. 

## 

### So let&#8217;s start!

Prerequisites  
**Software**  
        **&#8211;** Java 8 (8u92+)  
**&#8211;** Linux, Mac OS X, or other Unix-like OS **(Windows is not supported)**  
**Note:** If you use Windows, you can [install Virtual Machine](https://dev-guide.org/2020/02/07/how-to-set-up-a-virtual-machine/) on your device as I am using Ubuntu.  
  
[](https://druid.apache.org/docs/latest/tutorials/index.html#hardware)**Hardware**  
**&#8211;** 4 CPU  
**&#8211;** 16 GB or 8 GB RAM environment  
**&#8211;** Hard disk &#8211; 120 GB  


After installing the VM, open your favorite terminal and set up a few things.  
Check your java version. 

<pre class="wp-block-code"><code lang="bash" class="language-bash">java -version</code></pre>

If your java version is not Java 8 or it is missing, install it.

<pre class="wp-block-code"><code lang="bash" class="language-bash">sudo apt install openjdk-8-jdk-headless </code></pre>

Superset does not ship bundled with connectivity to databases, except for SQLite, which is part of the Python standard library. You’ll need to install the required packages for the database you want to use as your metadata database as well as the packages needed to connect to the databases you want to access through Superset. In our example, this database is Apache Druid. 

<pre class="wp-block-code"><code lang="bash" class="language-bash">sudo apt install python-pip
pip install pydruid</code></pre>

**Starting Druid**  
First, [download](http://mirrors.netix.net/apache/druid/0.17.0/apache-druid-0.17.0-bin.tar.gz) and extract the Druid distribution.

<pre class="wp-block-code"><code lang="bash" class="language-bash">tar -xzf apache-druid-0.17.0-bin.tar.gz
cd apache-druid-0.17.0</code></pre>

From the apache-druid-0.17.0 package root, run the following command:

<pre class="wp-block-code"><code lang="bash" class="language-bash">./bin/start-micro-quickstart</code></pre>

After running this command DO NOT close the terminal. Allow the process to charge.

**Starting Kafka**  
[Download](https://archive.apache.org/dist/kafka/2.1.0/kafka_2.12-2.1.0.tgz) Kafka then open a new terminal. Follow commands to extract the file and open the directory: 

<pre class="wp-block-code"><code lang="bash" class="language-bash">tar -xzf kafka_2.12-2.1.0.tgz
cd kafka_2.12-2.1.0</code></pre>

Start a Kafka broker by running the following command in a new terminal: 

<pre class="wp-block-code"><code lang="bash" class="language-bash">./bin/kafka-server-start.sh config/server.properties</code></pre>

After running this command DO NOT close the terminal. Allow the process to charge.  
Then in a new terminal run this command to create a Kafka topic called&nbsp;_wikipedia_, to which we&#8217;ll send data: 

<pre class="wp-block-code"><code lang="bash" class="language-bash">./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia</code></pre>

After the process is done, you can keep running your commands in this terminal. 

 **Load data into Kafka**  
Let&#8217;s launch a producer for our topic and send some data!  
Run the following command:

<pre class="wp-block-code"><code lang="bash" class="language-bash">cd apache-druid-0.17.0
cd quickstart/tutorial
gunzip -c wikiticker-2015-09-12-sampled.json.gz > wikiticker-2015-09-12-sampled.json</code></pre>

Run the following command, where **_{PATH\_TO\_DRUID}_** is replaced by the path to the Druid directory (e.g. **_/home/user/apache-druid-0.17.0_** ) :

<pre class="wp-block-code"><code lang="bash" class="language-bash">cd kafka_2.12-2.1.0
export KAFKA_OPTS="-Dfile.encoding=UTF-8"
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia &lt; {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json</code></pre>

The previous command posted sample events to the&nbsp;_wikipedia_&nbsp;Kafka topic. Now we will use Druid&#8217;s Kafka indexing service to ingest messages from our newly created topic. 

**Starting Superset**  
Superset stores database connection information in its metadata database. For that purpose, we use the&nbsp;`cryptography`&nbsp;Python library to encrypt connection passwords. Unfortunately, this library has OS level dependencies. Here’s how to install them:

For&nbsp;**Debian**&nbsp;and&nbsp;**Ubuntu**, the following command will ensure that the required dependencies are installed:

<pre class="wp-block-code"><code lang="bash" class="language-bash">sudo apt-get install build-essential libssl-dev libffi-dev python-dev python-pip libsasl2-dev libldap2-dev
sudo apt-get install build-essential libssl-dev libffi-dev python3.6-dev python-pip libsasl2-dev libldap2-dev</code></pre>

**Python virtualenv**  
**_It is recommended to install Superset inside a virtualenv._** Install virtualenv useing the next commands: 

<pre class="wp-block-code"><code lang="bash" class="language-bash">pip install virtualenv
python3 -m venv venv
. venv/bin/activate
pip install --upgrade setuptools pip</code></pre>

So now it is time to install Superset. Follow next commands:

<pre class="wp-block-code"><code lang="bash" class="language-bash">pip install apache-superset
superset db upgrade
$ export FLASK_APP=superset
flask fab create-admin</code></pre>

Whit the las command we want to create an admin user (you will be prompted to set a username, first and last name and e-mail before setting a password). Next command:

<pre class="wp-block-code"><code lang="bash" class="language-bash">superset load_examples
superset init
superset run -p 8088 --with-threads --reload --debugger</code></pre>

After that allow the process to charge.
